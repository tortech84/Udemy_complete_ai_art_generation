{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "002_CifarGAN",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Importing Python Packages for GAN"
      ],
      "metadata": {
        "id": "EXl0VNFc3AHF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GqVJURkN2hIo"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import cifar10, mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "!mkdir generated_images"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) Parameters for Neural Networks & Data"
      ],
      "metadata": {
        "id": "Qxz6PRB23yyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cifar10'dan gelen imajlar 32x32 olduğu için \n",
        "img_width = 32\n",
        "img_height = 32\n",
        "# rgb olduğu için 3 kanal\n",
        "channels = 3\n",
        "img_shape = (img_width, img_height, channels)\n",
        "latent_dim = 100\n",
        "adam = Adam(learning_rate=0.0002)"
      ],
      "metadata": {
        "id": "cP4XtUAz3rmS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) Building Generator"
      ],
      "metadata": {
        "id": "yQ6duytT7GrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_generator():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256 * 4 * 4, input_dim=latent_dim))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Reshape((4,4,256)))\n",
        "\n",
        "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    model.add(Conv2D(3, (3,3), activation='tanh', padding='same'))\n",
        "\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "generator = build_generator()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMEPpOj54dSz",
        "outputId": "1bb95fda-4360-471a-b3b0-2fc7ba639358"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_8 (Dense)             (None, 4096)              413696    \n",
            "                                                                 \n",
            " leaky_re_lu_23 (LeakyReLU)  (None, 4096)              0         \n",
            "                                                                 \n",
            " reshape_7 (Reshape)         (None, 4, 4, 256)         0         \n",
            "                                                                 \n",
            " conv2d_transpose_16 (Conv2D  (None, 8, 8, 128)        524416    \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " leaky_re_lu_24 (LeakyReLU)  (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_transpose_17 (Conv2D  (None, 16, 16, 128)      262272    \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " leaky_re_lu_25 (LeakyReLU)  (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_18 (Conv2D  (None, 32, 32, 128)      262272    \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " leaky_re_lu_26 (LeakyReLU)  (None, 32, 32, 128)       0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 32, 32, 3)         3459      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,466,115\n",
            "Trainable params: 1,466,115\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) Building Discriminator"
      ],
      "metadata": {
        "id": "eGpzjZ7ZE6oD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_discriminator():\n",
        "  # alexnet architecture\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(64, (3,3), padding='same', input_shape=img_shape))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "  model.add(Conv2D(128, (3,3), padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "  model.add(Conv2D(128, (3,3), padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "  model.add(Conv2D(256, (3,3), padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "  # squishes to one dimension (binary output)\n",
        "  model.add(Flatten())\n",
        "  # drops %40 percent of neurons and increases stability, generalization and decreases overfitting\n",
        "  model.add(Dropout(0.4))\n",
        "  # output layer, output is zero or one\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqtnoJSQD2Kp",
        "outputId": "dd09464f-2e66-442d-82ee-2cb1c7d346d4"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_20 (Conv2D)          (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " leaky_re_lu_45 (LeakyReLU)  (None, 32, 32, 64)        0         \n",
            "                                                                 \n",
            " conv2d_21 (Conv2D)          (None, 32, 32, 128)       73856     \n",
            "                                                                 \n",
            " leaky_re_lu_46 (LeakyReLU)  (None, 32, 32, 128)       0         \n",
            "                                                                 \n",
            " conv2d_22 (Conv2D)          (None, 32, 32, 128)       147584    \n",
            "                                                                 \n",
            " leaky_re_lu_47 (LeakyReLU)  (None, 32, 32, 128)       0         \n",
            "                                                                 \n",
            " conv2d_23 (Conv2D)          (None, 32, 32, 256)       295168    \n",
            "                                                                 \n",
            " leaky_re_lu_48 (LeakyReLU)  (None, 32, 32, 256)       0         \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 262144)            0         \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 262144)            0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1)                 262145    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 780,545\n",
            "Trainable params: 780,545\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5) Connecting Neural Network to "
      ],
      "metadata": {
        "id": "2AWBi1BBHglq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GAN = Sequential()\n",
        "discriminator.trainable = False\n",
        "GAN.add(generator)\n",
        "GAN.add(discriminator)\n",
        "\n",
        "GAN.compile(loss='binary_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "sDNxNBy2FUji"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6)"
      ],
      "metadata": {
        "id": "fr2Fnm7ZHp6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "## **7) Outputting Images**\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import imageio\n",
        "import PIL\n",
        "\n",
        "save_name = 0.00000000\n",
        "\n",
        "def save_imgs(epoch):\n",
        "  r, c = 5, 5\n",
        "  noise = np.random.normal(0, 1, (r * c, latent_dim))\n",
        "  gen_imgs = generator.predict(noise)\n",
        "  global save_name\n",
        "  save_name += 0.00000001\n",
        "  print(\"%.8f\" % save_name)\n",
        "\n",
        "  # Rescale images 0 - 1\n",
        "  gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "  fig, axs = plt.subplots(r, c)\n",
        "  cnt = 0\n",
        "  for i in range(r):\n",
        "    for j in range(c):\n",
        "      axs[i,j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n",
        "      axs[i,j].axis('off')\n",
        "      cnt += 1\n",
        "  fig.savefig(\"generated_images/%.8f.png\" % save_name)\n",
        "  print('saved')\n",
        "  plt.close()"
      ],
      "metadata": {
        "id": "trBWj-RvHrOj"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7) Training GAN"
      ],
      "metadata": {
        "id": "QNjQO15gHrqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epochs, batch_size=64, save_interval=200):\n",
        "    (X_train, _), (_, _) = cifar10.load_data()\n",
        "\n",
        "    # print(X_train.shape)\n",
        "    # Rescale data between -1 and 1\n",
        "    X_train = X_train / 127.5 -1.\n",
        "    bat_per_epo = int(X_train.shape[0] / batch_size)\n",
        "    # X_train = np.expand_dims(X_train, axis=3)\n",
        "    # print(X_train.shape)\n",
        "\n",
        "    # Create our Y for our Neural Networks\n",
        "    valid = np.ones((batch_size, 1))\n",
        "    fakes = np.zeros((batch_size, 1))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      for j in range(bat_per_epo):\n",
        "        # Get Random Batch\n",
        "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "        imgs = X_train[idx]\n",
        "\n",
        "        # Generate Fake Images\n",
        "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "        gen_imgs = generator.predict(noise)\n",
        "\n",
        "        # Train discriminator\n",
        "        d_loss_real = discrimiator.train_on_batch(imgs, valid)\n",
        "        d_loss_fake = discrimiator.train_on_batch(gen_imgs, fakes)\n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "\n",
        "        # inverse y label\n",
        "        g_loss = GAN.train_on_batch(noise, valid)\n",
        "\n",
        "        print(\"******** %d %d [D loss: %f, acc: %.2f%%] [G loss: %f]\" % (epoch, j, d_loss[0], 100* d_loss[1], g_loss))\n",
        "\n",
        "        #if(epoch % save_interval) == 0:\n",
        "      save_imgs(epoch)\n",
        "    \n",
        "    # print(valid)\n",
        "  \n",
        "train(30000, batch_size=64, save_interval=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eunFBcXHt26",
        "outputId": "42231d8f-8800-4755-a732-571085032036"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "******** 0 0 [D loss: 0.021702, acc: 99.22%] [G loss: 0.481260]\n",
            "******** 0 1 [D loss: 0.009510, acc: 100.00%] [G loss: 0.470114]\n",
            "******** 0 2 [D loss: 0.033336, acc: 98.44%] [G loss: 0.464242]\n",
            "******** 0 3 [D loss: 0.001308, acc: 100.00%] [G loss: 0.450176]\n",
            "******** 0 4 [D loss: 0.042191, acc: 99.22%] [G loss: 0.441333]\n",
            "******** 0 5 [D loss: 0.001981, acc: 100.00%] [G loss: 0.433809]\n",
            "******** 0 6 [D loss: 0.003996, acc: 100.00%] [G loss: 0.426990]\n",
            "******** 0 7 [D loss: 0.013271, acc: 100.00%] [G loss: 0.423956]\n",
            "******** 0 8 [D loss: 0.006674, acc: 100.00%] [G loss: 0.416771]\n",
            "******** 0 9 [D loss: 0.001329, acc: 100.00%] [G loss: 0.413816]\n",
            "******** 0 10 [D loss: 0.008394, acc: 99.22%] [G loss: 0.408796]\n",
            "******** 0 11 [D loss: 0.000263, acc: 100.00%] [G loss: 0.403910]\n",
            "******** 0 12 [D loss: 0.000186, acc: 100.00%] [G loss: 0.397427]\n",
            "******** 0 13 [D loss: 0.000213, acc: 100.00%] [G loss: 0.398254]\n",
            "******** 0 14 [D loss: 0.002068, acc: 100.00%] [G loss: 0.393268]\n",
            "******** 0 15 [D loss: 0.000050, acc: 100.00%] [G loss: 0.388341]\n",
            "******** 0 16 [D loss: 0.002073, acc: 100.00%] [G loss: 0.382355]\n",
            "******** 0 17 [D loss: 0.000039, acc: 100.00%] [G loss: 0.379917]\n",
            "******** 0 18 [D loss: 0.000017, acc: 100.00%] [G loss: 0.384585]\n",
            "******** 0 19 [D loss: 0.000015, acc: 100.00%] [G loss: 0.374800]\n",
            "******** 0 20 [D loss: 0.000019, acc: 100.00%] [G loss: 0.377550]\n",
            "******** 0 21 [D loss: 0.000192, acc: 100.00%] [G loss: 0.373924]\n",
            "******** 0 22 [D loss: 0.000057, acc: 100.00%] [G loss: 0.371843]\n",
            "******** 0 23 [D loss: 0.000019, acc: 100.00%] [G loss: 0.373249]\n",
            "******** 0 24 [D loss: 0.000026, acc: 100.00%] [G loss: 0.363598]\n",
            "******** 0 25 [D loss: 0.000015, acc: 100.00%] [G loss: 0.372799]\n",
            "******** 0 26 [D loss: 0.017471, acc: 99.22%] [G loss: 0.365682]\n",
            "******** 0 27 [D loss: 0.000148, acc: 100.00%] [G loss: 0.367067]\n",
            "******** 0 28 [D loss: 0.001917, acc: 100.00%] [G loss: 0.361431]\n",
            "******** 0 29 [D loss: 0.000079, acc: 100.00%] [G loss: 0.361137]\n",
            "******** 0 30 [D loss: 0.000058, acc: 100.00%] [G loss: 0.364845]\n",
            "******** 0 31 [D loss: 0.000067, acc: 100.00%] [G loss: 0.366630]\n",
            "******** 0 32 [D loss: 0.000094, acc: 100.00%] [G loss: 0.360035]\n",
            "******** 0 33 [D loss: 0.000091, acc: 100.00%] [G loss: 0.360783]\n",
            "******** 0 34 [D loss: 0.000625, acc: 100.00%] [G loss: 0.359310]\n",
            "******** 0 35 [D loss: 0.000105, acc: 100.00%] [G loss: 0.358830]\n",
            "******** 0 36 [D loss: 0.000107, acc: 100.00%] [G loss: 0.356135]\n",
            "******** 0 37 [D loss: 0.000100, acc: 100.00%] [G loss: 0.357395]\n",
            "******** 0 38 [D loss: 0.000109, acc: 100.00%] [G loss: 0.354678]\n",
            "******** 0 39 [D loss: 0.000100, acc: 100.00%] [G loss: 0.355944]\n",
            "******** 0 40 [D loss: 0.000099, acc: 100.00%] [G loss: 0.350860]\n",
            "******** 0 41 [D loss: 0.000101, acc: 100.00%] [G loss: 0.348126]\n",
            "******** 0 42 [D loss: 0.000096, acc: 100.00%] [G loss: 0.350669]\n",
            "******** 0 43 [D loss: 0.000089, acc: 100.00%] [G loss: 0.350911]\n",
            "******** 0 44 [D loss: 0.000088, acc: 100.00%] [G loss: 0.350778]\n",
            "******** 0 45 [D loss: 0.003940, acc: 100.00%] [G loss: 0.347229]\n",
            "******** 0 46 [D loss: 0.000416, acc: 100.00%] [G loss: 0.344682]\n",
            "******** 0 47 [D loss: 0.000101, acc: 100.00%] [G loss: 0.346329]\n",
            "******** 0 48 [D loss: 0.000093, acc: 100.00%] [G loss: 0.342039]\n",
            "******** 0 49 [D loss: 0.001511, acc: 100.00%] [G loss: 0.344846]\n",
            "******** 0 50 [D loss: 0.000109, acc: 100.00%] [G loss: 0.343357]\n",
            "******** 0 51 [D loss: 0.000103, acc: 100.00%] [G loss: 0.339937]\n",
            "******** 0 52 [D loss: 0.000098, acc: 100.00%] [G loss: 0.348364]\n",
            "******** 0 53 [D loss: 0.000099, acc: 100.00%] [G loss: 0.339456]\n",
            "******** 0 54 [D loss: 0.000105, acc: 100.00%] [G loss: 0.340628]\n",
            "******** 0 55 [D loss: 0.000323, acc: 100.00%] [G loss: 0.337055]\n",
            "******** 0 56 [D loss: 0.000097, acc: 100.00%] [G loss: 0.339593]\n",
            "******** 0 57 [D loss: 0.000538, acc: 100.00%] [G loss: 0.341437]\n",
            "******** 0 58 [D loss: 0.000796, acc: 100.00%] [G loss: 0.340935]\n",
            "******** 0 59 [D loss: 0.000099, acc: 100.00%] [G loss: 0.339931]\n",
            "******** 0 60 [D loss: 0.000124, acc: 100.00%] [G loss: 0.337020]\n",
            "******** 0 61 [D loss: 0.000095, acc: 100.00%] [G loss: 0.340126]\n",
            "******** 0 62 [D loss: 0.000089, acc: 100.00%] [G loss: 0.340765]\n",
            "******** 0 63 [D loss: 0.000083, acc: 100.00%] [G loss: 0.342098]\n",
            "******** 0 64 [D loss: 0.000083, acc: 100.00%] [G loss: 0.339111]\n",
            "******** 0 65 [D loss: 0.000074, acc: 100.00%] [G loss: 0.341507]\n",
            "******** 0 66 [D loss: 0.000071, acc: 100.00%] [G loss: 0.342504]\n",
            "******** 0 67 [D loss: 0.000066, acc: 100.00%] [G loss: 0.336553]\n",
            "******** 0 68 [D loss: 0.000058, acc: 100.00%] [G loss: 0.341708]\n",
            "******** 0 69 [D loss: 0.000053, acc: 100.00%] [G loss: 0.337857]\n",
            "******** 0 70 [D loss: 0.000065, acc: 100.00%] [G loss: 0.335660]\n",
            "******** 0 71 [D loss: 0.000043, acc: 100.00%] [G loss: 0.340176]\n",
            "******** 0 72 [D loss: 0.000040, acc: 100.00%] [G loss: 0.337008]\n",
            "******** 0 73 [D loss: 0.000035, acc: 100.00%] [G loss: 0.330788]\n",
            "******** 0 74 [D loss: 0.000032, acc: 100.00%] [G loss: 0.334891]\n",
            "******** 0 75 [D loss: 0.000300, acc: 100.00%] [G loss: 0.331310]\n",
            "******** 0 76 [D loss: 0.000029, acc: 100.00%] [G loss: 0.330933]\n",
            "******** 0 77 [D loss: 0.000027, acc: 100.00%] [G loss: 0.333447]\n",
            "******** 0 78 [D loss: 0.000026, acc: 100.00%] [G loss: 0.329710]\n",
            "******** 0 79 [D loss: 0.000027, acc: 100.00%] [G loss: 0.332964]\n",
            "******** 0 80 [D loss: 0.000026, acc: 100.00%] [G loss: 0.332625]\n",
            "******** 0 81 [D loss: 0.000031, acc: 100.00%] [G loss: 0.325804]\n",
            "******** 0 82 [D loss: 0.000025, acc: 100.00%] [G loss: 0.324410]\n",
            "******** 0 83 [D loss: 0.000021, acc: 100.00%] [G loss: 0.326222]\n",
            "******** 0 84 [D loss: 0.000020, acc: 100.00%] [G loss: 0.328416]\n",
            "******** 0 85 [D loss: 0.000023, acc: 100.00%] [G loss: 0.329892]\n",
            "******** 0 86 [D loss: 0.000020, acc: 100.00%] [G loss: 0.330412]\n",
            "******** 0 87 [D loss: 0.000019, acc: 100.00%] [G loss: 0.324806]\n",
            "******** 0 88 [D loss: 0.000018, acc: 100.00%] [G loss: 0.325423]\n",
            "******** 0 89 [D loss: 0.000018, acc: 100.00%] [G loss: 0.330931]\n",
            "******** 0 90 [D loss: 0.000016, acc: 100.00%] [G loss: 0.328133]\n",
            "******** 0 91 [D loss: 0.000016, acc: 100.00%] [G loss: 0.329768]\n",
            "******** 0 92 [D loss: 0.000014, acc: 100.00%] [G loss: 0.324265]\n",
            "******** 0 93 [D loss: 0.000014, acc: 100.00%] [G loss: 0.327523]\n",
            "******** 0 94 [D loss: 0.000014, acc: 100.00%] [G loss: 0.322672]\n",
            "******** 0 95 [D loss: 0.000014, acc: 100.00%] [G loss: 0.326833]\n",
            "******** 0 96 [D loss: 0.000268, acc: 100.00%] [G loss: 0.324483]\n",
            "******** 0 97 [D loss: 0.000018, acc: 100.00%] [G loss: 0.326219]\n",
            "******** 0 98 [D loss: 0.000015, acc: 100.00%] [G loss: 0.325101]\n",
            "******** 0 99 [D loss: 0.000012, acc: 100.00%] [G loss: 0.328805]\n",
            "******** 0 100 [D loss: 0.000276, acc: 100.00%] [G loss: 0.326073]\n",
            "******** 0 101 [D loss: 0.000039, acc: 100.00%] [G loss: 0.324675]\n",
            "******** 0 102 [D loss: 0.000013, acc: 100.00%] [G loss: 0.327225]\n",
            "******** 0 103 [D loss: 0.000015, acc: 100.00%] [G loss: 0.326462]\n",
            "******** 0 104 [D loss: 0.000013, acc: 100.00%] [G loss: 0.324599]\n",
            "******** 0 105 [D loss: 0.000784, acc: 100.00%] [G loss: 0.323295]\n",
            "******** 0 106 [D loss: 0.000014, acc: 100.00%] [G loss: 0.321415]\n",
            "******** 0 107 [D loss: 0.000015, acc: 100.00%] [G loss: 0.322818]\n",
            "******** 0 108 [D loss: 0.000018, acc: 100.00%] [G loss: 0.323593]\n",
            "******** 0 109 [D loss: 0.000020, acc: 100.00%] [G loss: 0.325338]\n",
            "******** 0 110 [D loss: 0.000018, acc: 100.00%] [G loss: 0.329321]\n",
            "******** 0 111 [D loss: 0.000016, acc: 100.00%] [G loss: 0.321566]\n",
            "******** 0 112 [D loss: 0.000017, acc: 100.00%] [G loss: 0.327378]\n",
            "******** 0 113 [D loss: 0.000020, acc: 100.00%] [G loss: 0.324890]\n",
            "******** 0 114 [D loss: 0.000019, acc: 100.00%] [G loss: 0.328126]\n",
            "******** 0 115 [D loss: 0.000016, acc: 100.00%] [G loss: 0.325093]\n",
            "******** 0 116 [D loss: 0.000016, acc: 100.00%] [G loss: 0.322284]\n",
            "******** 0 117 [D loss: 0.000016, acc: 100.00%] [G loss: 0.322899]\n",
            "******** 0 118 [D loss: 0.000016, acc: 100.00%] [G loss: 0.325440]\n",
            "******** 0 119 [D loss: 0.000015, acc: 100.00%] [G loss: 0.325146]\n",
            "******** 0 120 [D loss: 0.000015, acc: 100.00%] [G loss: 0.325935]\n",
            "******** 0 121 [D loss: 0.000015, acc: 100.00%] [G loss: 0.319196]\n",
            "******** 0 122 [D loss: 0.000014, acc: 100.00%] [G loss: 0.323981]\n",
            "******** 0 123 [D loss: 0.000013, acc: 100.00%] [G loss: 0.321591]\n",
            "******** 0 124 [D loss: 0.000013, acc: 100.00%] [G loss: 0.320534]\n",
            "******** 0 125 [D loss: 0.000013, acc: 100.00%] [G loss: 0.323501]\n",
            "******** 0 126 [D loss: 0.000013, acc: 100.00%] [G loss: 0.324445]\n",
            "******** 0 127 [D loss: 0.000085, acc: 100.00%] [G loss: 0.324874]\n",
            "******** 0 128 [D loss: 0.000027, acc: 100.00%] [G loss: 0.323120]\n",
            "******** 0 129 [D loss: 0.000012, acc: 100.00%] [G loss: 0.320522]\n",
            "******** 0 130 [D loss: 0.000011, acc: 100.00%] [G loss: 0.324799]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FLf0Luh4Kug9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}