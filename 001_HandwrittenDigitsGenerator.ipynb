{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fCEDCU_qrC0"
      },
      "source": [
        "<p><img alt=\"Colaboratory logo\" height=\"45px\" src=\"/img/colab_favicon.ico\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "\n",
        "<h1>Colaboratory nedir?</h1>\n",
        "\n",
        "Colaboratory &#40;ya da kısaca \"Colab\"&#41;, tarayıcınızda Python'u yazmanızı ve çalıştırmanızı sağlar. Üstelik: \n",
        "- Hiç yapılandırma gerektirmez\n",
        "- GPU'lara ücretsiz erişim imkanı sunar\n",
        "- Kolay paylaşım imkanı sunar\n",
        "\n",
        "İster <strong>öğrenci</strong> ister <strong>veri bilimci</strong> ister <strong>yapay zeka araştırmacısı</strong> olun, Colab işinizi kolaylaştırabilir. Daha fazla bilgi edinmek için <a href=\"https://www.youtube.com/watch?v=inN8seMm7UI\">Colab'e Giriş</a> videosunu izleyebilir ya da aşağıdan hemen kullanmaya başlayabilirsiniz."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Importing Python Packages for GAN"
      ],
      "metadata": {
        "id": "dqtRG8GrMUZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Dense, Reshape, Flatten\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import numpy as np\n",
        "!mkdir generated_images"
      ],
      "metadata": {
        "id": "qFK36zMRLjmf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) Variables for Neural Networks & Data"
      ],
      "metadata": {
        "id": "6AmKXf1yMcfx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# image width and height for mnist handwritten digits\n",
        "img_width = 28\n",
        "img_height = 28\n",
        "\n",
        "# Grayscale is 1 color channel, RGB is 3 color channel\n",
        "channels = 1 \n",
        "\n",
        "img_shape = (img_width, img_height, channels)\n",
        "\n",
        "# 100 random values - noise\n",
        "latent_dim = 100 \n",
        "\n",
        "# Adam is an optimizer that implements Adam algorithm, Adam optimization is a stochastic gradiend descent method\n",
        "# lr - learning rate - how fast learning occurs\n",
        "adam = Adam(lr=0.0001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcyG8ot-NFap",
        "outputId": "b9d49f52-6a24-4d95-c557-e5c79106a785"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) Building Generator"
      ],
      "metadata": {
        "id": "AtnB3qYEVS2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_generator():\n",
        "  model = Sequential()\n",
        "\n",
        "  # adds layer which has 256 output neurons and our noise (latent_dim)\n",
        "  model.add(Dense(256, input_dim=latent_dim))\n",
        "\n",
        "  # activation layer\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "  # increases stability of GAN\n",
        "  model.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "  model.add(Dense(512))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "  model.add(Dense(1024))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "  # output shape like our input shape 28x28\n",
        "  model.add(Dense(np.prod(img_shape), activation='tanh'))\n",
        "  model.add(Reshape(img_shape))\n",
        "\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "generator = build_generator()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZl8oNSwVVgi",
        "outputId": "3ef28799-3475-48fb-c939-1e17207c9390"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_14 (Dense)            (None, 256)               25856     \n",
            "                                                                 \n",
            " leaky_re_lu_9 (LeakyReLU)   (None, 256)               0         \n",
            "                                                                 \n",
            " batch_normalization_9 (Batc  (None, 256)              1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 512)               131584    \n",
            "                                                                 \n",
            " leaky_re_lu_10 (LeakyReLU)  (None, 512)               0         \n",
            "                                                                 \n",
            " batch_normalization_10 (Bat  (None, 512)              2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 1024)              525312    \n",
            "                                                                 \n",
            " leaky_re_lu_11 (LeakyReLU)  (None, 1024)              0         \n",
            "                                                                 \n",
            " batch_normalization_11 (Bat  (None, 1024)             4096      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 784)               803600    \n",
            "                                                                 \n",
            " reshape_1 (Reshape)         (None, 28, 28, 1)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,493,520\n",
            "Trainable params: 1,489,936\n",
            "Non-trainable params: 3,584\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) Building Discriminator"
      ],
      "metadata": {
        "id": "gx5o7eWhnqE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_discriminator():\n",
        "  model = Sequential()\n",
        "\n",
        "  # squishes img_shape (output of generator method)\n",
        "  model.add(Flatten(input_shape=img_shape))\n",
        "  model.add(Dense(512))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Dense(256))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  \n",
        "  # sonuçları artırmak için ek katmanlar (layer) kullanılabilir ancak modelin çalışması için daha fazla eğitim gerekecektir.\n",
        "  # model.add(Dense(256))\n",
        "  # model.add(LeakyReLU(alpha=0.2))\n",
        "  # model.add(Dense(256))\n",
        "  # model.add(LeakyReLU(alpha=0.2))\n",
        "  # model.add(Dense(256))\n",
        "  # model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "discriminator = build_discriminator()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsxE17GmkU-p",
        "outputId": "22ccd770-8678-43c2-b9e1-705003f30efc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_1 (Flatten)         (None, 784)               0         \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 512)               401920    \n",
            "                                                                 \n",
            " leaky_re_lu_14 (LeakyReLU)  (None, 512)               0         \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 256)               131328    \n",
            "                                                                 \n",
            " leaky_re_lu_15 (LeakyReLU)  (None, 256)               0         \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 533,505\n",
            "Trainable params: 533,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5) Connectin Neural Networks to Build GAN"
      ],
      "metadata": {
        "id": "CrD5Or4hpVsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loss parameter is used for loss function gradient descent and binary crossentropy is used because our output is binary\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
        "\n",
        "GAN = Sequential()\n",
        "discriminator.trainable = False\n",
        "GAN.add(generator)\n",
        "GAN.add(discriminator)\n",
        "\n",
        "GAN.compile(loss='binary_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "QHhwwXU4o0_l"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6) Outputting Images"
      ],
      "metadata": {
        "id": "G46d4575t6O3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "## **7) Outputting Images**\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import imageio\n",
        "import PIL\n",
        "\n",
        "save_name = 0.00000000\n",
        "\n",
        "def save_imgs(epoch):\n",
        "  r, c = 5, 5\n",
        "  noise = np.random.normal(0, 1, (r * c, latent_dim))\n",
        "  gen_imgs = generator.predict(noise)\n",
        "  global save_name\n",
        "  save_name += 0.00000001\n",
        "  print(\"%.8f\" % save_name)\n",
        "\n",
        "  # Rescale images 0 - 1\n",
        "  gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "  fig, axs = plt.subplots(r, c)\n",
        "  cnt = 0\n",
        "  for i in range(r):\n",
        "    for j in range(c):\n",
        "      axs[i,j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n",
        "      axs[i,j].axis('off')\n",
        "      cnt += 1\n",
        "  fig.savefig(\"generated_images/%.8f.png\" % save_name)\n",
        "  print('saved')\n",
        "  plt.close()\n"
      ],
      "metadata": {
        "id": "EzKSCz82sIzl"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7) Training GAN"
      ],
      "metadata": {
        "id": "wlXkx7lWqZQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# batch_size bir seferde neural network e girilen imaj sayısı, # 60000 giriş imaj için ideal, 1000 imaj için 16, 30000 imaj içinse 32 kullanılabilir\n",
        "# save_interval - kayıt aralığı\n",
        "def train(epochs, batch_size=64, save_interval=200): \n",
        "  # verileri yüklediğimizde (x_train, y_train), (x_test, y_test) şeklinde bir yükleme yapılır, \n",
        "  # x_train (60000, 28, 28) 60000 adet ve her biri 28x28 olan imajları ifade eder\n",
        "  # y_train yüklenen imajlara ilişkin etiket (label) verilerini içerir. Buradaki değer mesela x_train içine eklenmiş olan el ile yazılmış (hand written) 1 rakamının 1 olan karşılığı olup,\n",
        "  # modelimizin hedefi de bu veriyi elde etmektir.\n",
        "  # x_test ve y_test de test verilerimizdir ancak GAN ile uğraştığımız için bu verilere de ihtiyacımız yoktur\n",
        "  (X_train, _), (_, _) = mnist.load_data()\n",
        "  \n",
        "  # for normalizing our data, this squish it to a small form valued between -1 and 1, bu sayede training işlemi daha performaslı olacaktır.\n",
        "  X_train = X_train / 127.5 -1.\n",
        "\n",
        "  # y value that we want from neural network to predict \n",
        "  # valid vector has full of (=64 (batch_size)) ones in it\n",
        "  valid = np.ones((batch_size, 1))\n",
        "  # vector full of zeroes\n",
        "  fakes = np.zeros((batch_size, 1))\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    # takes a random image collection (64 image batch)\n",
        "    idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "    \n",
        "    # loads 64 random images to imgs\n",
        "    imgs = X_train[idx]\n",
        "\n",
        "    # get 64 noisy images\n",
        "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "    gen_imgs = generator.predict(noise)\n",
        "\n",
        "    # Train The discriminator\n",
        "    d_loss_real = discriminator.train_on_batch(imgs, valid)\n",
        "    d_loss_fake = discriminator.train_on_batch(gen_imgs, fakes)\n",
        "    # gerçek imajlarla geçerli (valid) ve sahte imajlarla sahte olduğu (noise) öğretilen discriminator ın kayıp ortalaması bulunur\n",
        "    d_loss = np.add(d_loss_real, d_loss_fake) * 0.5\n",
        "\n",
        "    # generator ve discriminator ın aynı gürültü ile üretilmemesi için farklı bir gürültü üretilir, 64 adet latent_dim (gürültü) üretilir \n",
        "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "\n",
        "    # Training The GAN, inverse y label\n",
        "    g_loss = GAN.train_on_batch(noise, valid)\n",
        "\n",
        "    print(\"************** %d [D loss: %f, acc: %.2f] [G loss: %f]\" % (epoch, d_loss[0], d_loss[1] * 100, g_loss))\n",
        "\n",
        "  if(epoch % save_interval) == 0:\n",
        "    save_imgs(epoch)\n",
        "\n",
        "  print(X_train.shape)\n",
        "\n",
        "train(400, batch_size=64, save_interval=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXztz59Bqbwt",
        "outputId": "e7d3035c-5270-44f0-c6bd-17b337a421a3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "************** 0 [D loss: 0.015769, acc: 99.22] [G loss: 6.794882]\n",
            "************** 1 [D loss: 0.241749, acc: 92.19] [G loss: 3.790228]\n",
            "************** 2 [D loss: 0.655665, acc: 78.91] [G loss: 1.864378]\n",
            "************** 3 [D loss: 1.652482, acc: 64.06] [G loss: 0.549068]\n",
            "************** 4 [D loss: 2.241443, acc: 56.25] [G loss: 0.174404]\n",
            "************** 5 [D loss: 3.288042, acc: 50.78] [G loss: 0.039990]\n",
            "************** 6 [D loss: 3.691968, acc: 50.78] [G loss: 0.007407]\n",
            "************** 7 [D loss: 4.571404, acc: 49.22] [G loss: 0.011484]\n",
            "************** 8 [D loss: 4.628678, acc: 50.00] [G loss: 0.004112]\n",
            "************** 9 [D loss: 4.747078, acc: 49.22] [G loss: 0.001222]\n",
            "************** 10 [D loss: 5.319050, acc: 50.00] [G loss: 0.001623]\n",
            "************** 11 [D loss: 4.906177, acc: 50.00] [G loss: 0.008386]\n",
            "************** 12 [D loss: 4.970658, acc: 50.78] [G loss: 0.000987]\n",
            "************** 13 [D loss: 5.030145, acc: 50.00] [G loss: 0.001788]\n",
            "************** 14 [D loss: 5.305889, acc: 50.00] [G loss: 0.025062]\n",
            "************** 15 [D loss: 6.044608, acc: 50.00] [G loss: 0.000630]\n",
            "************** 16 [D loss: 5.741637, acc: 49.22] [G loss: 0.000452]\n",
            "************** 17 [D loss: 5.596209, acc: 51.56] [G loss: 0.000350]\n",
            "************** 18 [D loss: 5.563442, acc: 50.00] [G loss: 0.002529]\n",
            "************** 19 [D loss: 5.446219, acc: 50.00] [G loss: 0.022353]\n",
            "************** 20 [D loss: 6.651803, acc: 50.78] [G loss: 0.001433]\n",
            "************** 21 [D loss: 5.727417, acc: 50.78] [G loss: 0.001336]\n",
            "************** 22 [D loss: 6.113734, acc: 50.00] [G loss: 0.000490]\n",
            "************** 23 [D loss: 5.741383, acc: 49.22] [G loss: 0.065499]\n",
            "************** 24 [D loss: 5.619112, acc: 50.78] [G loss: 0.001557]\n",
            "************** 25 [D loss: 5.528673, acc: 50.00] [G loss: 0.000832]\n",
            "************** 26 [D loss: 5.895991, acc: 50.00] [G loss: 0.000476]\n",
            "************** 27 [D loss: 5.194438, acc: 50.00] [G loss: 0.073259]\n",
            "************** 28 [D loss: 5.948941, acc: 50.00] [G loss: 0.000835]\n",
            "************** 29 [D loss: 5.891609, acc: 50.00] [G loss: 0.006131]\n",
            "************** 30 [D loss: 6.619594, acc: 50.00] [G loss: 0.002590]\n",
            "************** 31 [D loss: 5.561671, acc: 51.56] [G loss: 0.003238]\n",
            "************** 32 [D loss: 5.823121, acc: 50.78] [G loss: 0.033831]\n",
            "************** 33 [D loss: 5.862585, acc: 51.56] [G loss: 0.003638]\n",
            "************** 34 [D loss: 6.333929, acc: 50.78] [G loss: 0.011900]\n",
            "************** 35 [D loss: 6.864501, acc: 51.56] [G loss: 0.132930]\n",
            "************** 36 [D loss: 6.475258, acc: 49.22] [G loss: 0.001134]\n",
            "************** 37 [D loss: 6.395479, acc: 52.34] [G loss: 0.012501]\n",
            "************** 38 [D loss: 6.106554, acc: 50.00] [G loss: 0.004425]\n",
            "************** 39 [D loss: 6.151866, acc: 50.78] [G loss: 0.423247]\n",
            "************** 40 [D loss: 6.513183, acc: 50.78] [G loss: 0.001805]\n",
            "************** 41 [D loss: 6.377687, acc: 50.78] [G loss: 0.002394]\n",
            "************** 42 [D loss: 5.973645, acc: 50.00] [G loss: 0.008713]\n",
            "************** 43 [D loss: 6.442097, acc: 50.00] [G loss: 0.001053]\n",
            "************** 44 [D loss: 7.199657, acc: 50.00] [G loss: 0.000511]\n",
            "************** 45 [D loss: 6.645206, acc: 50.00] [G loss: 0.000471]\n",
            "************** 46 [D loss: 6.660981, acc: 49.22] [G loss: 0.196437]\n",
            "************** 47 [D loss: 6.420858, acc: 49.22] [G loss: 0.063140]\n",
            "************** 48 [D loss: 5.778686, acc: 50.78] [G loss: 0.002266]\n",
            "************** 49 [D loss: 6.574655, acc: 50.78] [G loss: 0.003565]\n",
            "************** 50 [D loss: 6.156006, acc: 50.78] [G loss: 0.002300]\n",
            "************** 51 [D loss: 6.387950, acc: 50.00] [G loss: 0.005715]\n",
            "************** 52 [D loss: 6.335824, acc: 50.78] [G loss: 0.000905]\n",
            "************** 53 [D loss: 6.687056, acc: 50.00] [G loss: 0.000283]\n",
            "************** 54 [D loss: 6.711333, acc: 50.00] [G loss: 0.003937]\n",
            "************** 55 [D loss: 6.399760, acc: 50.00] [G loss: 0.001137]\n",
            "************** 56 [D loss: 6.268306, acc: 50.78] [G loss: 0.000267]\n",
            "************** 57 [D loss: 6.085713, acc: 50.00] [G loss: 0.000658]\n",
            "************** 58 [D loss: 6.721327, acc: 49.22] [G loss: 0.000748]\n",
            "************** 59 [D loss: 6.516000, acc: 50.00] [G loss: 0.001582]\n",
            "************** 60 [D loss: 5.996848, acc: 49.22] [G loss: 0.007513]\n",
            "************** 61 [D loss: 6.356916, acc: 50.00] [G loss: 0.001215]\n",
            "************** 62 [D loss: 6.575382, acc: 50.00] [G loss: 0.136309]\n",
            "************** 63 [D loss: 6.258181, acc: 50.00] [G loss: 0.000314]\n",
            "************** 64 [D loss: 6.602933, acc: 50.00] [G loss: 0.000430]\n",
            "************** 65 [D loss: 7.162836, acc: 49.22] [G loss: 0.000389]\n",
            "************** 66 [D loss: 6.504990, acc: 49.22] [G loss: 0.001635]\n",
            "************** 67 [D loss: 6.589918, acc: 50.78] [G loss: 0.000938]\n",
            "************** 68 [D loss: 6.114044, acc: 50.78] [G loss: 0.000285]\n",
            "************** 69 [D loss: 6.611117, acc: 50.00] [G loss: 0.000304]\n",
            "************** 70 [D loss: 6.788346, acc: 50.78] [G loss: 0.000624]\n",
            "************** 71 [D loss: 6.177366, acc: 50.78] [G loss: 0.062279]\n",
            "************** 72 [D loss: 6.342574, acc: 50.00] [G loss: 0.003119]\n",
            "************** 73 [D loss: 7.129268, acc: 50.00] [G loss: 0.000394]\n",
            "************** 74 [D loss: 6.938711, acc: 50.00] [G loss: 0.000814]\n",
            "************** 75 [D loss: 6.280825, acc: 50.00] [G loss: 0.000165]\n",
            "************** 76 [D loss: 6.471236, acc: 50.00] [G loss: 0.000184]\n",
            "************** 77 [D loss: 6.209092, acc: 50.00] [G loss: 0.000145]\n",
            "************** 78 [D loss: 6.088168, acc: 50.78] [G loss: 0.000228]\n",
            "************** 79 [D loss: 6.874115, acc: 49.22] [G loss: 0.000122]\n",
            "************** 80 [D loss: 6.793264, acc: 49.22] [G loss: 0.000180]\n",
            "************** 81 [D loss: 7.010023, acc: 50.78] [G loss: 0.000300]\n",
            "************** 82 [D loss: 6.729345, acc: 49.22] [G loss: 0.000135]\n",
            "************** 83 [D loss: 6.655595, acc: 50.00] [G loss: 0.000748]\n",
            "************** 84 [D loss: 6.751045, acc: 49.22] [G loss: 0.000575]\n",
            "************** 85 [D loss: 6.352924, acc: 50.78] [G loss: 0.000481]\n",
            "************** 86 [D loss: 6.642832, acc: 50.00] [G loss: 0.000155]\n",
            "************** 87 [D loss: 6.354369, acc: 50.78] [G loss: 0.000295]\n",
            "************** 88 [D loss: 6.708677, acc: 50.00] [G loss: 0.004056]\n",
            "************** 89 [D loss: 6.673182, acc: 50.00] [G loss: 0.000153]\n",
            "************** 90 [D loss: 5.685781, acc: 50.78] [G loss: 0.000187]\n",
            "************** 91 [D loss: 6.693177, acc: 50.00] [G loss: 0.000119]\n",
            "************** 92 [D loss: 7.574163, acc: 48.44] [G loss: 0.004658]\n",
            "************** 93 [D loss: 6.903548, acc: 50.00] [G loss: 0.000160]\n",
            "************** 94 [D loss: 6.393759, acc: 49.22] [G loss: 0.012915]\n",
            "************** 95 [D loss: 6.685000, acc: 49.22] [G loss: 0.000423]\n",
            "************** 96 [D loss: 6.633013, acc: 50.00] [G loss: 0.001851]\n",
            "************** 97 [D loss: 7.267048, acc: 50.00] [G loss: 0.000271]\n",
            "************** 98 [D loss: 6.816301, acc: 49.22] [G loss: 0.000131]\n",
            "************** 99 [D loss: 7.399721, acc: 49.22] [G loss: 0.000113]\n",
            "************** 100 [D loss: 7.039719, acc: 50.00] [G loss: 0.000104]\n",
            "************** 101 [D loss: 6.692983, acc: 50.00] [G loss: 0.000216]\n",
            "************** 102 [D loss: 6.147151, acc: 50.00] [G loss: 0.000079]\n",
            "************** 103 [D loss: 6.830393, acc: 50.00] [G loss: 0.000190]\n",
            "************** 104 [D loss: 6.476143, acc: 49.22] [G loss: 0.000153]\n",
            "************** 105 [D loss: 6.215910, acc: 50.00] [G loss: 0.000107]\n",
            "************** 106 [D loss: 6.033510, acc: 50.00] [G loss: 0.000067]\n",
            "************** 107 [D loss: 6.780041, acc: 50.00] [G loss: 0.000102]\n",
            "************** 108 [D loss: 7.090565, acc: 50.00] [G loss: 0.000596]\n",
            "************** 109 [D loss: 7.029102, acc: 50.00] [G loss: 0.000094]\n",
            "************** 110 [D loss: 6.763615, acc: 50.00] [G loss: 0.000091]\n",
            "************** 111 [D loss: 6.613439, acc: 50.00] [G loss: 0.000127]\n",
            "************** 112 [D loss: 7.288451, acc: 50.00] [G loss: 0.000128]\n",
            "************** 113 [D loss: 6.762503, acc: 50.00] [G loss: 0.000239]\n",
            "************** 114 [D loss: 6.624501, acc: 50.00] [G loss: 0.000070]\n",
            "************** 115 [D loss: 6.904794, acc: 50.00] [G loss: 0.000092]\n",
            "************** 116 [D loss: 6.980004, acc: 50.00] [G loss: 0.000332]\n",
            "************** 117 [D loss: 7.262536, acc: 50.00] [G loss: 0.000093]\n",
            "************** 118 [D loss: 7.358415, acc: 50.00] [G loss: 0.000174]\n",
            "************** 119 [D loss: 7.413900, acc: 49.22] [G loss: 0.000089]\n",
            "************** 120 [D loss: 7.247930, acc: 50.00] [G loss: 0.000114]\n",
            "************** 121 [D loss: 7.180272, acc: 50.00] [G loss: 0.000213]\n",
            "************** 122 [D loss: 7.523306, acc: 49.22] [G loss: 0.000128]\n",
            "************** 123 [D loss: 6.905522, acc: 50.00] [G loss: 0.000123]\n",
            "************** 124 [D loss: 7.430946, acc: 50.00] [G loss: 0.000098]\n",
            "************** 125 [D loss: 7.050260, acc: 50.00] [G loss: 0.000116]\n",
            "************** 126 [D loss: 7.145147, acc: 50.00] [G loss: 0.000096]\n",
            "************** 127 [D loss: 7.479316, acc: 50.00] [G loss: 0.000096]\n",
            "************** 128 [D loss: 6.604556, acc: 50.00] [G loss: 0.000122]\n",
            "************** 129 [D loss: 6.712474, acc: 50.00] [G loss: 0.000079]\n",
            "************** 130 [D loss: 6.720414, acc: 50.00] [G loss: 0.000080]\n",
            "************** 131 [D loss: 6.973029, acc: 50.00] [G loss: 0.000204]\n",
            "************** 132 [D loss: 7.089030, acc: 49.22] [G loss: 0.000139]\n",
            "************** 133 [D loss: 6.981113, acc: 49.22] [G loss: 0.000122]\n",
            "************** 134 [D loss: 7.325036, acc: 49.22] [G loss: 0.000122]\n",
            "************** 135 [D loss: 6.942601, acc: 50.00] [G loss: 0.000139]\n",
            "************** 136 [D loss: 6.258828, acc: 50.00] [G loss: 0.000082]\n",
            "************** 137 [D loss: 6.707871, acc: 49.22] [G loss: 0.000094]\n",
            "************** 138 [D loss: 6.988130, acc: 49.22] [G loss: 0.000090]\n",
            "************** 139 [D loss: 6.728246, acc: 50.00] [G loss: 0.000125]\n",
            "************** 140 [D loss: 6.977427, acc: 50.00] [G loss: 0.000080]\n",
            "************** 141 [D loss: 6.922217, acc: 48.44] [G loss: 0.000083]\n",
            "************** 142 [D loss: 7.400878, acc: 50.00] [G loss: 0.000125]\n",
            "************** 143 [D loss: 7.697025, acc: 50.00] [G loss: 0.000076]\n",
            "************** 144 [D loss: 6.538075, acc: 50.00] [G loss: 0.000087]\n",
            "************** 145 [D loss: 6.520834, acc: 49.22] [G loss: 0.000097]\n",
            "************** 146 [D loss: 6.707834, acc: 49.22] [G loss: 0.000054]\n",
            "************** 147 [D loss: 6.849942, acc: 50.00] [G loss: 0.000059]\n",
            "************** 148 [D loss: 7.344887, acc: 50.00] [G loss: 0.000067]\n",
            "************** 149 [D loss: 7.464541, acc: 50.78] [G loss: 0.000074]\n",
            "************** 150 [D loss: 6.744197, acc: 50.00] [G loss: 0.000082]\n",
            "************** 151 [D loss: 6.901575, acc: 50.00] [G loss: 0.000135]\n",
            "************** 152 [D loss: 6.915267, acc: 50.00] [G loss: 0.000080]\n",
            "************** 153 [D loss: 7.496978, acc: 50.00] [G loss: 0.000079]\n",
            "************** 154 [D loss: 7.079239, acc: 50.00] [G loss: 0.000091]\n",
            "************** 155 [D loss: 6.948323, acc: 50.00] [G loss: 0.000073]\n",
            "************** 156 [D loss: 6.909075, acc: 49.22] [G loss: 0.000131]\n",
            "************** 157 [D loss: 7.146256, acc: 50.00] [G loss: 0.000075]\n",
            "************** 158 [D loss: 6.781536, acc: 50.00] [G loss: 0.000105]\n",
            "************** 159 [D loss: 7.112651, acc: 49.22] [G loss: 0.000447]\n",
            "************** 160 [D loss: 7.502731, acc: 50.00] [G loss: 0.000066]\n",
            "************** 161 [D loss: 7.096837, acc: 50.00] [G loss: 0.000153]\n",
            "************** 162 [D loss: 7.157374, acc: 50.00] [G loss: 0.000078]\n",
            "************** 163 [D loss: 6.815736, acc: 50.00] [G loss: 0.000065]\n",
            "************** 164 [D loss: 7.306741, acc: 50.00] [G loss: 0.000101]\n",
            "************** 165 [D loss: 7.411468, acc: 50.00] [G loss: 0.000111]\n",
            "************** 166 [D loss: 7.016307, acc: 50.00] [G loss: 0.000080]\n",
            "************** 167 [D loss: 7.677332, acc: 49.22] [G loss: 0.000053]\n",
            "************** 168 [D loss: 7.453574, acc: 50.00] [G loss: 0.000061]\n",
            "************** 169 [D loss: 7.157831, acc: 50.00] [G loss: 0.000059]\n",
            "************** 170 [D loss: 7.268117, acc: 49.22] [G loss: 0.000055]\n",
            "************** 171 [D loss: 6.874399, acc: 50.00] [G loss: 0.000065]\n",
            "************** 172 [D loss: 7.832349, acc: 50.00] [G loss: 0.000065]\n",
            "************** 173 [D loss: 6.571667, acc: 50.00] [G loss: 0.000067]\n",
            "************** 174 [D loss: 6.899753, acc: 50.00] [G loss: 0.000061]\n",
            "************** 175 [D loss: 7.582338, acc: 50.00] [G loss: 0.000095]\n",
            "************** 176 [D loss: 6.947527, acc: 50.00] [G loss: 0.000051]\n",
            "************** 177 [D loss: 6.062175, acc: 50.00] [G loss: 0.000043]\n",
            "************** 178 [D loss: 6.937763, acc: 50.00] [G loss: 0.000096]\n",
            "************** 179 [D loss: 6.674574, acc: 50.00] [G loss: 0.000117]\n",
            "************** 180 [D loss: 7.139478, acc: 48.44] [G loss: 0.000056]\n",
            "************** 181 [D loss: 6.851101, acc: 50.00] [G loss: 0.000058]\n",
            "************** 182 [D loss: 7.603676, acc: 49.22] [G loss: 0.000251]\n",
            "************** 183 [D loss: 7.165126, acc: 50.00] [G loss: 0.000061]\n",
            "************** 184 [D loss: 6.531457, acc: 50.00] [G loss: 0.000148]\n",
            "************** 185 [D loss: 6.500730, acc: 49.22] [G loss: 0.000089]\n",
            "************** 186 [D loss: 7.405422, acc: 48.44] [G loss: 0.000049]\n",
            "************** 187 [D loss: 6.999284, acc: 50.00] [G loss: 0.000049]\n",
            "************** 188 [D loss: 7.022615, acc: 49.22] [G loss: 0.000044]\n",
            "************** 189 [D loss: 7.312007, acc: 50.00] [G loss: 0.000302]\n",
            "************** 190 [D loss: 7.321897, acc: 49.22] [G loss: 0.000051]\n",
            "************** 191 [D loss: 6.831421, acc: 50.00] [G loss: 0.000056]\n",
            "************** 192 [D loss: 7.325199, acc: 49.22] [G loss: 0.000067]\n",
            "************** 193 [D loss: 7.321611, acc: 49.22] [G loss: 0.000045]\n",
            "************** 194 [D loss: 6.918670, acc: 50.00] [G loss: 0.000049]\n",
            "************** 195 [D loss: 6.684874, acc: 49.22] [G loss: 0.000043]\n",
            "************** 196 [D loss: 7.514018, acc: 50.00] [G loss: 0.000060]\n",
            "************** 197 [D loss: 7.148298, acc: 50.78] [G loss: 0.000050]\n",
            "************** 198 [D loss: 7.327734, acc: 50.00] [G loss: 0.000051]\n",
            "************** 199 [D loss: 7.743542, acc: 50.00] [G loss: 0.000063]\n",
            "************** 200 [D loss: 7.218599, acc: 50.00] [G loss: 0.000049]\n",
            "************** 201 [D loss: 7.082504, acc: 50.00] [G loss: 0.000055]\n",
            "************** 202 [D loss: 7.509296, acc: 50.00] [G loss: 0.000043]\n",
            "************** 203 [D loss: 7.260624, acc: 49.22] [G loss: 0.000050]\n",
            "************** 204 [D loss: 7.551668, acc: 50.00] [G loss: 0.000051]\n",
            "************** 205 [D loss: 7.557379, acc: 49.22] [G loss: 0.000085]\n",
            "************** 206 [D loss: 7.156585, acc: 50.00] [G loss: 0.000057]\n",
            "************** 207 [D loss: 7.518274, acc: 50.00] [G loss: 0.000123]\n",
            "************** 208 [D loss: 7.177981, acc: 49.22] [G loss: 0.000063]\n",
            "************** 209 [D loss: 7.030337, acc: 50.00] [G loss: 0.000063]\n",
            "************** 210 [D loss: 7.992886, acc: 50.00] [G loss: 0.000943]\n",
            "************** 211 [D loss: 7.087530, acc: 50.00] [G loss: 0.000046]\n",
            "************** 212 [D loss: 7.010017, acc: 50.00] [G loss: 0.000050]\n",
            "************** 213 [D loss: 7.000742, acc: 50.00] [G loss: 0.000046]\n",
            "************** 214 [D loss: 6.549284, acc: 50.00] [G loss: 0.000048]\n",
            "************** 215 [D loss: 7.569485, acc: 50.00] [G loss: 0.000039]\n",
            "************** 216 [D loss: 7.228835, acc: 50.00] [G loss: 0.000046]\n",
            "************** 217 [D loss: 7.197713, acc: 50.00] [G loss: 0.000050]\n",
            "************** 218 [D loss: 7.092109, acc: 50.00] [G loss: 0.000119]\n",
            "************** 219 [D loss: 6.870668, acc: 50.00] [G loss: 0.000034]\n",
            "************** 220 [D loss: 7.208287, acc: 50.00] [G loss: 0.000058]\n",
            "************** 221 [D loss: 7.302790, acc: 50.00] [G loss: 0.000037]\n",
            "************** 222 [D loss: 7.263580, acc: 50.00] [G loss: 0.000045]\n",
            "************** 223 [D loss: 7.831644, acc: 50.00] [G loss: 0.000049]\n",
            "************** 224 [D loss: 7.504711, acc: 50.00] [G loss: 0.000042]\n",
            "************** 225 [D loss: 7.350501, acc: 50.00] [G loss: 0.000056]\n",
            "************** 226 [D loss: 6.710513, acc: 50.00] [G loss: 0.000045]\n",
            "************** 227 [D loss: 7.503494, acc: 49.22] [G loss: 0.000047]\n",
            "************** 228 [D loss: 7.619863, acc: 50.00] [G loss: 0.000057]\n",
            "************** 229 [D loss: 7.164333, acc: 50.00] [G loss: 0.000037]\n",
            "************** 230 [D loss: 6.831627, acc: 50.00] [G loss: 0.000043]\n",
            "************** 231 [D loss: 6.479490, acc: 50.00] [G loss: 0.002140]\n",
            "************** 232 [D loss: 7.250988, acc: 50.00] [G loss: 0.000046]\n",
            "************** 233 [D loss: 6.549599, acc: 50.00] [G loss: 0.000077]\n",
            "************** 234 [D loss: 6.618537, acc: 50.00] [G loss: 0.000041]\n",
            "************** 235 [D loss: 7.154319, acc: 50.00] [G loss: 0.000039]\n",
            "************** 236 [D loss: 7.289863, acc: 49.22] [G loss: 0.000036]\n",
            "************** 237 [D loss: 7.085591, acc: 50.00] [G loss: 0.000034]\n",
            "************** 238 [D loss: 7.775092, acc: 50.00] [G loss: 0.000036]\n",
            "************** 239 [D loss: 7.190376, acc: 50.00] [G loss: 0.000039]\n",
            "************** 240 [D loss: 7.117277, acc: 49.22] [G loss: 0.000031]\n",
            "************** 241 [D loss: 7.048413, acc: 50.00] [G loss: 0.000036]\n",
            "************** 242 [D loss: 6.921939, acc: 49.22] [G loss: 0.000034]\n",
            "************** 243 [D loss: 7.069472, acc: 49.22] [G loss: 0.000038]\n",
            "************** 244 [D loss: 7.020778, acc: 50.00] [G loss: 0.000043]\n",
            "************** 245 [D loss: 7.352277, acc: 49.22] [G loss: 0.000035]\n",
            "************** 246 [D loss: 7.659557, acc: 50.00] [G loss: 0.000046]\n",
            "************** 247 [D loss: 7.235903, acc: 50.00] [G loss: 0.000036]\n",
            "************** 248 [D loss: 6.797139, acc: 50.00] [G loss: 0.000044]\n",
            "************** 249 [D loss: 6.540403, acc: 50.00] [G loss: 0.000057]\n",
            "************** 250 [D loss: 7.161211, acc: 50.00] [G loss: 0.000031]\n",
            "************** 251 [D loss: 7.003494, acc: 49.22] [G loss: 0.000034]\n",
            "************** 252 [D loss: 6.953654, acc: 50.00] [G loss: 0.000027]\n",
            "************** 253 [D loss: 7.375633, acc: 50.00] [G loss: 0.000039]\n",
            "************** 254 [D loss: 6.887360, acc: 50.00] [G loss: 0.000035]\n",
            "************** 255 [D loss: 7.175532, acc: 50.00] [G loss: 0.000036]\n",
            "************** 256 [D loss: 7.183256, acc: 50.00] [G loss: 0.000220]\n",
            "************** 257 [D loss: 7.187433, acc: 50.00] [G loss: 0.000035]\n",
            "************** 258 [D loss: 6.970251, acc: 50.00] [G loss: 0.000037]\n",
            "************** 259 [D loss: 6.769637, acc: 50.00] [G loss: 0.000036]\n",
            "************** 260 [D loss: 7.067392, acc: 50.00] [G loss: 0.000034]\n",
            "************** 261 [D loss: 7.174453, acc: 49.22] [G loss: 0.000030]\n",
            "************** 262 [D loss: 7.405867, acc: 50.00] [G loss: 0.000034]\n",
            "************** 263 [D loss: 7.330283, acc: 49.22] [G loss: 0.000028]\n",
            "************** 264 [D loss: 7.094649, acc: 50.00] [G loss: 0.000039]\n",
            "************** 265 [D loss: 7.522399, acc: 50.00] [G loss: 0.000033]\n",
            "************** 266 [D loss: 6.901341, acc: 50.00] [G loss: 0.000027]\n",
            "************** 267 [D loss: 7.063652, acc: 50.00] [G loss: 0.000028]\n",
            "************** 268 [D loss: 7.568449, acc: 50.00] [G loss: 0.000031]\n",
            "************** 269 [D loss: 7.215767, acc: 50.00] [G loss: 0.000027]\n",
            "************** 270 [D loss: 7.634540, acc: 50.00] [G loss: 0.000030]\n",
            "************** 271 [D loss: 7.294561, acc: 50.00] [G loss: 0.000153]\n",
            "************** 272 [D loss: 7.337613, acc: 50.00] [G loss: 0.000028]\n",
            "************** 273 [D loss: 7.446463, acc: 50.00] [G loss: 0.000034]\n",
            "************** 274 [D loss: 7.877378, acc: 50.00] [G loss: 0.000024]\n",
            "************** 275 [D loss: 8.325708, acc: 50.00] [G loss: 0.000028]\n",
            "************** 276 [D loss: 7.275340, acc: 50.00] [G loss: 0.000031]\n",
            "************** 277 [D loss: 6.967624, acc: 50.00] [G loss: 0.000028]\n",
            "************** 278 [D loss: 7.853891, acc: 49.22] [G loss: 0.000030]\n",
            "************** 279 [D loss: 7.300193, acc: 50.00] [G loss: 0.000036]\n",
            "************** 280 [D loss: 7.250549, acc: 50.00] [G loss: 0.000034]\n",
            "************** 281 [D loss: 7.135344, acc: 49.22] [G loss: 0.000026]\n",
            "************** 282 [D loss: 7.549045, acc: 50.00] [G loss: 0.000028]\n",
            "************** 283 [D loss: 7.265767, acc: 50.00] [G loss: 0.000030]\n",
            "************** 284 [D loss: 6.760014, acc: 50.00] [G loss: 0.000027]\n",
            "************** 285 [D loss: 7.437414, acc: 50.00] [G loss: 0.000026]\n",
            "************** 286 [D loss: 7.658921, acc: 49.22] [G loss: 0.000027]\n",
            "************** 287 [D loss: 6.818969, acc: 50.00] [G loss: 0.000029]\n",
            "************** 288 [D loss: 7.522995, acc: 49.22] [G loss: 0.000031]\n",
            "************** 289 [D loss: 7.116138, acc: 50.00] [G loss: 0.000030]\n",
            "************** 290 [D loss: 7.573712, acc: 49.22] [G loss: 0.000028]\n",
            "************** 291 [D loss: 7.556649, acc: 50.00] [G loss: 0.000037]\n",
            "************** 292 [D loss: 7.716839, acc: 50.00] [G loss: 0.000023]\n",
            "************** 293 [D loss: 6.820191, acc: 50.00] [G loss: 0.000041]\n",
            "************** 294 [D loss: 7.200131, acc: 50.00] [G loss: 0.000029]\n",
            "************** 295 [D loss: 7.227705, acc: 50.00] [G loss: 0.000029]\n",
            "************** 296 [D loss: 7.215285, acc: 49.22] [G loss: 0.000030]\n",
            "************** 297 [D loss: 6.843287, acc: 48.44] [G loss: 0.000024]\n",
            "************** 298 [D loss: 6.813484, acc: 50.00] [G loss: 0.000025]\n",
            "************** 299 [D loss: 7.152250, acc: 50.00] [G loss: 0.000037]\n",
            "************** 300 [D loss: 7.573291, acc: 50.00] [G loss: 0.000030]\n",
            "************** 301 [D loss: 7.852116, acc: 49.22] [G loss: 0.000025]\n",
            "************** 302 [D loss: 7.555146, acc: 49.22] [G loss: 0.000034]\n",
            "************** 303 [D loss: 7.722509, acc: 50.00] [G loss: 0.000035]\n",
            "************** 304 [D loss: 7.498192, acc: 50.00] [G loss: 0.000031]\n",
            "************** 305 [D loss: 7.440923, acc: 50.00] [G loss: 0.000030]\n",
            "************** 306 [D loss: 7.401660, acc: 50.78] [G loss: 0.000032]\n",
            "************** 307 [D loss: 7.695368, acc: 50.00] [G loss: 0.000022]\n",
            "************** 308 [D loss: 7.015690, acc: 50.00] [G loss: 0.000040]\n",
            "************** 309 [D loss: 7.657094, acc: 47.66] [G loss: 0.000025]\n",
            "************** 310 [D loss: 7.115087, acc: 49.22] [G loss: 0.000029]\n",
            "************** 311 [D loss: 7.270778, acc: 50.00] [G loss: 0.000025]\n",
            "************** 312 [D loss: 7.528147, acc: 50.00] [G loss: 0.000022]\n",
            "************** 313 [D loss: 7.047176, acc: 49.22] [G loss: 0.000025]\n",
            "************** 314 [D loss: 7.438949, acc: 50.00] [G loss: 0.000026]\n",
            "************** 315 [D loss: 7.353310, acc: 50.00] [G loss: 0.000023]\n",
            "************** 316 [D loss: 7.542950, acc: 50.00] [G loss: 0.000025]\n",
            "************** 317 [D loss: 7.096960, acc: 49.22] [G loss: 0.000019]\n",
            "************** 318 [D loss: 7.520434, acc: 50.00] [G loss: 0.000068]\n",
            "************** 319 [D loss: 7.535358, acc: 49.22] [G loss: 0.000057]\n",
            "************** 320 [D loss: 7.205362, acc: 49.22] [G loss: 0.000025]\n",
            "************** 321 [D loss: 7.051687, acc: 50.00] [G loss: 0.000026]\n",
            "************** 322 [D loss: 7.557592, acc: 50.00] [G loss: 0.000027]\n",
            "************** 323 [D loss: 7.489565, acc: 50.00] [G loss: 0.000028]\n",
            "************** 324 [D loss: 7.201578, acc: 49.22] [G loss: 0.000022]\n",
            "************** 325 [D loss: 7.550176, acc: 50.00] [G loss: 0.000020]\n",
            "************** 326 [D loss: 7.327757, acc: 50.00] [G loss: 0.000023]\n",
            "************** 327 [D loss: 7.090141, acc: 50.00] [G loss: 0.000047]\n",
            "************** 328 [D loss: 7.292261, acc: 50.00] [G loss: 0.000036]\n",
            "************** 329 [D loss: 7.076488, acc: 49.22] [G loss: 0.000024]\n",
            "************** 330 [D loss: 6.571861, acc: 50.00] [G loss: 0.000026]\n",
            "************** 331 [D loss: 7.203427, acc: 50.00] [G loss: 0.000023]\n",
            "************** 332 [D loss: 7.382841, acc: 49.22] [G loss: 0.000021]\n",
            "************** 333 [D loss: 7.452367, acc: 50.00] [G loss: 0.000057]\n",
            "************** 334 [D loss: 7.975071, acc: 50.00] [G loss: 0.000052]\n",
            "************** 335 [D loss: 7.307988, acc: 50.00] [G loss: 0.000022]\n",
            "************** 336 [D loss: 7.745786, acc: 50.00] [G loss: 0.000022]\n",
            "************** 337 [D loss: 7.487286, acc: 50.00] [G loss: 0.000024]\n",
            "************** 338 [D loss: 6.958478, acc: 50.00] [G loss: 0.000026]\n",
            "************** 339 [D loss: 7.153775, acc: 50.00] [G loss: 0.000021]\n",
            "************** 340 [D loss: 7.404586, acc: 49.22] [G loss: 0.000023]\n",
            "************** 341 [D loss: 7.046873, acc: 50.00] [G loss: 0.000021]\n",
            "************** 342 [D loss: 7.069106, acc: 50.00] [G loss: 0.000023]\n",
            "************** 343 [D loss: 7.477399, acc: 49.22] [G loss: 0.000020]\n",
            "************** 344 [D loss: 7.608442, acc: 49.22] [G loss: 0.000026]\n",
            "************** 345 [D loss: 7.332729, acc: 50.00] [G loss: 0.000024]\n",
            "************** 346 [D loss: 7.269187, acc: 50.00] [G loss: 0.000020]\n",
            "************** 347 [D loss: 7.316866, acc: 50.00] [G loss: 0.000023]\n",
            "************** 348 [D loss: 8.036138, acc: 50.00] [G loss: 0.000020]\n",
            "************** 349 [D loss: 7.368656, acc: 50.00] [G loss: 0.000021]\n",
            "************** 350 [D loss: 7.195147, acc: 49.22] [G loss: 0.000022]\n",
            "************** 351 [D loss: 7.717919, acc: 49.22] [G loss: 0.000021]\n",
            "************** 352 [D loss: 7.405884, acc: 49.22] [G loss: 0.000020]\n",
            "************** 353 [D loss: 6.903254, acc: 50.00] [G loss: 0.000023]\n",
            "************** 354 [D loss: 7.260513, acc: 50.00] [G loss: 0.000023]\n",
            "************** 355 [D loss: 6.810137, acc: 50.00] [G loss: 0.000019]\n",
            "************** 356 [D loss: 7.173168, acc: 50.00] [G loss: 0.000023]\n",
            "************** 357 [D loss: 8.084981, acc: 50.00] [G loss: 0.000023]\n",
            "************** 358 [D loss: 7.165662, acc: 50.00] [G loss: 0.000021]\n",
            "************** 359 [D loss: 7.379857, acc: 50.00] [G loss: 0.000019]\n",
            "************** 360 [D loss: 7.676219, acc: 50.00] [G loss: 0.000029]\n",
            "************** 361 [D loss: 7.764420, acc: 50.00] [G loss: 0.000022]\n",
            "************** 362 [D loss: 7.481789, acc: 47.66] [G loss: 0.000024]\n",
            "************** 363 [D loss: 7.372213, acc: 50.00] [G loss: 0.000023]\n",
            "************** 364 [D loss: 7.718668, acc: 49.22] [G loss: 0.000024]\n",
            "************** 365 [D loss: 7.207027, acc: 50.00] [G loss: 0.000028]\n",
            "************** 366 [D loss: 7.054539, acc: 49.22] [G loss: 0.000021]\n",
            "************** 367 [D loss: 7.248179, acc: 49.22] [G loss: 0.000024]\n",
            "************** 368 [D loss: 8.048618, acc: 50.00] [G loss: 0.000022]\n",
            "************** 369 [D loss: 7.441913, acc: 50.00] [G loss: 0.000019]\n",
            "************** 370 [D loss: 7.551001, acc: 50.00] [G loss: 0.000030]\n",
            "************** 371 [D loss: 7.570026, acc: 50.00] [G loss: 0.000019]\n",
            "************** 372 [D loss: 7.418874, acc: 50.00] [G loss: 0.000022]\n",
            "************** 373 [D loss: 6.847670, acc: 50.00] [G loss: 0.000019]\n",
            "************** 374 [D loss: 7.549441, acc: 50.00] [G loss: 0.000021]\n",
            "************** 375 [D loss: 7.369918, acc: 50.00] [G loss: 0.000032]\n",
            "************** 376 [D loss: 7.317068, acc: 50.00] [G loss: 0.000021]\n",
            "************** 377 [D loss: 6.862458, acc: 50.00] [G loss: 0.000024]\n",
            "************** 378 [D loss: 7.570730, acc: 50.00] [G loss: 0.000018]\n",
            "************** 379 [D loss: 6.835237, acc: 49.22] [G loss: 0.000020]\n",
            "************** 380 [D loss: 7.499064, acc: 50.00] [G loss: 0.000019]\n",
            "************** 381 [D loss: 7.671844, acc: 50.00] [G loss: 0.000024]\n",
            "************** 382 [D loss: 7.125224, acc: 50.00] [G loss: 0.000017]\n",
            "************** 383 [D loss: 7.654599, acc: 49.22] [G loss: 0.000019]\n",
            "************** 384 [D loss: 7.709866, acc: 50.00] [G loss: 0.000020]\n",
            "************** 385 [D loss: 7.846322, acc: 50.00] [G loss: 0.000019]\n",
            "************** 386 [D loss: 7.360538, acc: 50.00] [G loss: 0.000022]\n",
            "************** 387 [D loss: 7.628972, acc: 49.22] [G loss: 0.000017]\n",
            "************** 388 [D loss: 7.560609, acc: 47.66] [G loss: 0.000022]\n",
            "************** 389 [D loss: 7.380945, acc: 50.00] [G loss: 0.000017]\n",
            "************** 390 [D loss: 7.239514, acc: 50.00] [G loss: 0.000020]\n",
            "************** 391 [D loss: 7.424585, acc: 50.00] [G loss: 0.000020]\n",
            "************** 392 [D loss: 7.568280, acc: 50.00] [G loss: 0.000017]\n",
            "************** 393 [D loss: 7.777858, acc: 49.22] [G loss: 0.000034]\n",
            "************** 394 [D loss: 7.054040, acc: 50.00] [G loss: 0.000020]\n",
            "************** 395 [D loss: 7.353043, acc: 49.22] [G loss: 0.000021]\n",
            "************** 396 [D loss: 7.144435, acc: 50.00] [G loss: 0.000017]\n",
            "************** 397 [D loss: 7.782162, acc: 49.22] [G loss: 0.000014]\n",
            "************** 398 [D loss: 7.034969, acc: 47.66] [G loss: 0.000020]\n",
            "************** 399 [D loss: 7.246204, acc: 50.00] [G loss: 0.000051]\n",
            "(60000, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8) Making GIF"
      ],
      "metadata": {
        "id": "5La75LaGwq4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display a single image using the epoch number\n",
        "# def display_image(epoch_no):\n",
        "#   return PIL.Image.open('generated_images/%.8f.png'.format(epoch_no))\n",
        "\n",
        "anim_file = 'scgan.gif'\n",
        "\n",
        "with imageio.get_writer(anim_file, mode='I') as writer:\n",
        "  filenames = glob.glob('generated_images/*.png')\n",
        "  filenames = sorted(filenames)\n",
        "  for filename in filenames:\n",
        "    image = imageio.imread(filename)\n",
        "    writer.append_data(image)"
      ],
      "metadata": {
        "id": "QAo7-WmuvnFJ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PooBkvu_x4Ry"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Colaboratory'ye Hoş Geldiniz",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}